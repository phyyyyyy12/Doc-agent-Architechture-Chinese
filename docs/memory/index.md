# Memory 模块：动态 Token 窗口与语义压缩管理

1. **初始化**: 获取所有历史消息。
2. **计算资源**: 
   $$可用 Token = 总窗口 - 预留空间 - 当前输入$$
3. **分级策略**:
   * **策略 1 (全量)**: 若 Token 充足，直接返回全部历史。
   * **策略 2 (分层)**: 
       * **近场 (近2轮)**: 100% 完整保留，确保对话连贯性。
       * **远场 (更早消息)**: 进入压缩链路。
   * **策略 3 (压缩)**:
       * 优先使用 **LLM 语义压缩**（提取精华）。
       * 失败则回退至 **简单文本截断/压缩**。
       * **缓存**: 压缩后的摘要会被缓存以节省二次消耗。
4. **输出**: 最终组装 “近场全量 + 远场摘要” 返回给模型。

## 🌟 核心特性 / Key Features

### 1. 动态 Token 窗口管理 (Dynamic Token Window)

不再依赖不稳定的“固定轮数”策略，而是基于模型 Context Window 进行实时精算：

* **实时配额监控：** 每一轮对话前自动计算已用 Token 和剩余空间。
* **锚点保护 (Anchor Protection)：** 强制固定 `System Prompt`，确保 Agent 的核心指令在任何极端情况下都不会被挤出上下文。

### 2. 差异化留存策略 (Tiered Retention)

针对 RAG 场景设计了非对称的记忆留存逻辑：

* **近场全量 (Full-Text)：** 仅保留最近 2 轮对话的原始文本，维持即时的对话语气与流畅度。
* **远场压缩 (Summary)：** 2 轮之前的历史记录自动触发 **Summary 压缩**，将冗长的对话转为精炼的语义线索。

### 3. 检索轨迹清理 (Retrieval Trace Cleaning)

专门针对工具返回的“脏数据”进行处理：

* **噪声过滤：** 自动识别并剔除工具返回的原始调试日志、冗余错误信息（如 "Information not found"）。
* **语义蒸馏：** 对检索到的 MD 文档片段进行去冗余处理，只向模型输送最核心的知识点，防止上下文污染。

---

## 🛠️ 记忆结构示例 / Memory Structure

处理后的上下文构建逻辑如下：

| 层次 | 处理方式 | 包含内容 |
| --- | --- | --- |
| **System 层** | **永久保留** | 核心角色设定、任务指令、操作约束 |
| **远场记忆** | **语义摘要** | 前序轮次的任务目标、已确认的事实结论 |
| **近场记忆** | **完整文本** | 用户最近的提问、Agent 最近的详细回答 |
| **工具缓存** | **精简注入** | 经过清洗后的 MD 分块、必要的元数据路径 |
